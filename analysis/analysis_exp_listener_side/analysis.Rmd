---
title: "analysis"
output: html_document
---

# Preamble: Packages & global options

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache   = TRUE,
  echo    = TRUE,
  warning = FALSE,
  message = FALSE
)
```

Loading necessary packages:
```{r}
rm(list = ls())
library(ggplot2)
library(dplyr)
library(tidyr)
library(aida)
library(broom)
library(MASS)
library(brms)
library(cmdstanr)
library(stringr)
library(cspplot) # https://github.com/CogSciPrag/cspplot
```

Setting some global options:

```{r, global-options}

# these options help Stan run faster
options(mc.cores = parallel::detectCores(),
        brms.backend = "cmdstanr")

# use the CSP-theme for plotting
theme_set(theme_csp())

# global color scheme from CSP
project_colors = cspplot::list_colors() |> pull(hex)

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
  scale_fill_manual(..., values = project_colors)
}


```


# Data inspection & preprocessing

## Load data
```{r}
data <- read.csv("../../data/data_listenerside/data_pilot1.csv")
list_info <- read.csv("../../experiments/listener_side/items/final_listener_items.csv")
```

```{r}
# Add list info to the data, allowing by-item level analysis
list_info %>% mutate(
    item_id = row_number(),
    list = if_else(item_id %% 2 == 1, 1L, 2L),
    # Make the first letter of Q1 uppercase
    Q1 = str_replace(Q1, "^([a-zA-Z])", str_to_upper),

    # Ensure A ends with a period
    A = if_else(str_detect(A, "\\.$"), A, paste0(A, "."))
  ) -> list_info

data <- data %>%
  left_join(
    list_info %>%
      dplyr::select(Q1, Q2, A, condition, item_id, list),
    by = c(
      "Q1",
      "Q2",
      "A",
      "condition"
    )
  )
```


## Basic data

### Participants info

```{r}
# How many participants do we have?
num_participants <- data %>% distinct(submission_id)
print("We have the following number of participants:")
nrow(num_participants)
```


## Descriptive data analysis
```{r}

experiment_condition <- c("low", "info", "high")

# Filter out NA data and sample trial data
data <- data %>%
  mutate(condition = factor(condition, levels = experiment_condition)) %>%
  filter(!is.na(response)) %>%
  filter(condition %in% experiment_condition)

# Encode the responses into conditions
response_code <- c(
  "Student"   = "low",
  "Teacher"   = "high",
  "Principal" = "info"
)

data <- data %>%
  mutate(
    response_condition = response_code[response],
    response_condition = factor(response_condition,
                                levels = experiment_condition)
  )

# Compute count and proportions by condition
descriptive_summary <- data %>%
  group_by(condition, response_condition) %>%
  summarise(
    n = n(),
    .groups = "drop"
  ) %>%
  group_by(condition) %>%
  mutate(
    proportion = n / sum(n)
  ) %>%
  ungroup()

descriptive_summary
```
```{r}
descriptive_wide <- descriptive_summary %>%
  dplyr::select(condition, response_condition, proportion) %>%
  pivot_wider(
    names_from  = response_condition,
    values_from = proportion,
    values_fill = 0
  )

descriptive_wide
```

Sanity check.
```{r}
# all responses mapped?
sum(is.na(data$response_condition))

# proportions sum to 1?
descriptive_summary %>%
  group_by(condition) %>%
  summarise(sum_prop = sum(proportion))
```

```{r}
# Helper for bootstrapping
bootstrap_props_one_condition <- function(df, B = 2000, level = 0.95) {
  # Ensure factor with stable levels
  res_levels <- levels(df$response_condition)
  if (is.null(res_levels)) {
    res_levels <- sort(unique(df$response_condition))
    df <- df %>% mutate(response_condition = factor(response_condition,
                                                    levels = res_levels))
  }
  
  boot_mat <- replicate(B, {
    idx <- sample(seq_len(nrow(df)), replace = TRUE)
    tab <- table(df$response_condition[idx])
    # ensure all levels present
    tab_full <- tab[res_levels]
    tab_full[is.na(tab_full)] <- 0
    as.numeric(tab_full) / length(idx)
  })
  
  alpha <- (1 - level) / 2
  ci_low  <- apply(boot_mat, 1, quantile, probs = alpha)
  ci_high <- apply(boot_mat, 1, quantile, probs = 1 - alpha)
  prop_emp <- as.numeric(table(df$response_condition)[res_levels]) / nrow(df)
  
  tibble(
    response_condition = factor(res_levels, levels = res_levels),
    proportion         = prop_emp,
    ci_low             = ci_low,
    ci_high            = ci_high
  )
}

# Bootstrap proportions with CIs by condition
boot_descriptive <- data %>%
  group_by(condition) %>%
  group_modify(~ bootstrap_props_one_condition(.x)) %>%
  ungroup()

boot_descriptive

# Plot descriptive proportions with CIs
ggplot(boot_descriptive,
       aes(x = condition,
           y = proportion,
           fill = response_condition)) +
  geom_col(position = position_dodge(width = 0.9)) +
  geom_errorbar(
    aes(ymin = ci_low, ymax = ci_high),
    position = position_dodge(width = 0.9),
    width = 0.2
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                     limits = c(0, 1)) +
  labs(
    x = "Experimental condition",
    y = "Proportion of responses",
    fill = "Response type"
  ) +
  theme_csp()
```
By-item analysis.
```{r}
# Plot descriptive proportions with CIs
data %>% filter(condition == "info") %>%
  group_by(item_id, response_condition) %>%
  summarise(
    n = n(),
    .groups = "drop"
  ) %>%
  group_by(item_id) %>%
  mutate(
    proportion = n / sum(n)
  ) %>%
  ungroup() %>%
  ggplot(aes(x = factor(item_id),
             y = proportion,
             fill = response_condition)) +
  geom_col(position = position_dodge(width = 0.9)) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1),
                     limits = c(0, 1)) +
  labs(
    x = "Item ID",
    y = "Proportion of responses",
    fill = "Response type",
    title = "Descriptive by-item analysis for condition 'info'"
  ) +
  theme_csp()

#17  most  some  wrong      [12, 9, 9, 9, 9]      info          NaN         NaN   -1.042818
#18  most   all  wrong     [12, 12, 0, 0, 0]      info          NaN         NaN   -1.116629
#19  some   all  wrong    [12, 12, 12, 0, 0]      info          NaN         NaN   -1.026854
#20  some   all  right     [12, 12, 0, 0, 0]      info          NaN         NaN   -1.027021
```
## Inferential data analysis

First we try with GLM based analysis. We could for each conditions enocode fit and non-fit.

```{r}
# 1. Fit vs non-fit per trial
data <- data %>%
  mutate(
    fit = if_else(condition == response_condition, 1L, 0L),
    # set reference level (change if you prefer another baseline)
    condition = relevel(condition, ref = "info")
  )

# 2. Descriptive proportions of 'fit' by condition
fit_summary <- data %>%
  group_by(condition) %>%
  summarise(
    n      = n(),
    n_fit  = sum(fit),
    prop_fit = n_fit / n,
    .groups = "drop"
  )

fit_summary
```

```{r}
# Logistic regression: fit ~ condition
glm_fit <- glm(fit ~ condition,
               data   = data,
               family = binomial)

summary(glm_fit)

# Odds ratios with 95% CIs
glm_fit_or <- tidy(glm_fit,
                   conf.int   = TRUE,
                   exponentiate = TRUE)

glm_fit_or
```
Next we try with binomial test:
```{r}
chance <- 1/3

fit_tests <- fit_summary %>%
  rowwise() %>%
  mutate(
    bt = list(binom.test(n_fit, n, p = chance)),
    p_value  = bt$p.value,
    ci_low   = bt$conf.int[1],
    ci_high  = bt$conf.int[2]
  ) %>%
  ungroup() %>%
  dplyr::select(condition, n, n_fit, prop_fit, p_value, ci_low, ci_high)

fit_tests
```

Finally we will try an ordinal logit regression analysis:
```{r}


# Make sure factors are set up correctly
data <- data %>%
  mutate(
    # ordered outcome for brms
    response_ord = factor(
      response_condition,
      levels = c("low", "info", "high"),
      ordered = TRUE
    ),
    # set baseline of condition
    condition = relevel(condition, ref = "info")
  )

table(data$response_ord, data$condition)
```

```{r}
# Priors: fairly weakly informative
priors <- c(
  set_prior("normal(0, 2)", class = "b"),         # effects of condition
  set_prior("normal(0, 5)", class = "Intercept")  # thresholds (cut-points)
)

ord_brms <- brm(
  formula = response_ord ~ condition,
  family  = cumulative("logit"),
  data    = data,
  prior   = priors,
  chains  = 4,
  iter    = 4000,
  warmup  = 2000,
  cores   = 4,
  seed    = 1234,
  backend = "cmdstanr"
)
```


```{r, results = 'hide'}
summary(ord_brms)
```

```{r}
fixef(ord_brms)  # raw posterior summaries on log-odds scale

# Exponentiate to get odds ratios
fixef_ord_or <- exp(fixef(ord_brms)[, c("Estimate", "Q2.5", "Q97.5")])
fixef_ord_or
```
```{r}
new_dat <- data.frame(
  condition = factor(c("info", "high", "low"),
                     levels = levels(data$condition))
)

pred_probs <- fitted(
  ord_brms,
  newdata = new_dat,
  summary = TRUE  # average across posterior draws
)

pred_probs
```

# Alternative Bayesian data analysis

## Conjunctive hypothesis testing with Bayesian multinomial model

The main prediction we have is that answer category `X` should be  the modal choice in condition `X`. 
For `X` to be the modal choice means to be more frequently chosen than any other option `Y`.
Since there are three choice options in each condition, we essentially deal with a *conjunctive hypothesis*, namely that `X > Y1` *and* `X > Y2`.
Concretely, we want to test the following three hypotheses:

1. in condition `low`, answer category `low` is more likely than either of the two others
2. in condition `info`, answer category `info` is more likely than either of the two others
3. in condition `high`, answer category `high` is more likely than either of the two others

To do this, we run a single Bayesian model with `brms` and then compute the posterior probabilities of the above hypotheses based on samples from the posterior distribution.

First, prepare the data by coding whether a response matches the predictions about the modal response in each condition. 
We code the remaining two options as `non-match-1` or `non-match-2`, where the latter is the "opposite" to the modal response in conditions `low` and `high` (which is relevant for the subsequent, more focused analysis):

```{r, BDA-data-preparation}
# code the data as 'match', 'non-match-1', 'non-match-2', depending on whether 
# the response matches the condition or not
data_brm <-  data |> 
  mutate(condition = relevel(condition, ref = "low"),
         response_condition = case_when(
           response_condition == condition  ~ "match",
           response_condition == "low"  & condition == "info"  ~ "non-match-1",
           response_condition == "low"  & condition == "high"  ~ "non-match-2",
           response_condition == "info" & condition == "low"   ~ "non-match-1",
           response_condition == "info" & condition == "high"  ~ "non-match-2",
           response_condition == "high" & condition == "info"  ~ "non-match-1",
           response_condition == "high" & condition == "low"   ~ "non-match-2"
         ))
```


```{r, BDA-brms-fit, results = 'hide'}
# run a multinomial Bayesian regression model with by-subjects random intercepts and slopes
fit <- brm(
  formula = response_condition ~ condition + (1 + condition | submission_id),
  family  = categorical(),
  data    = data_brm,
  chains  = 4,
  iter    = 4000,
  warmup  = 2000,
  backend = "cmdstanr"
)
  
```

Next, we extract posterior samples using the `tidybayes::add_epred_draws` function, which gives us samples from the posterior distribution of the expected values of the each response category for each condition.
Then we extract the relevant posterior probabilities of the conjunctive hypotheses by computing the proportion of posterior samples in which the `match` category is more likely than both `non-match-1` and `non-match-2`. 

```{r, BDA-posteriors}
data_brm |> 
  dplyr::select(condition) |> 
  unique() |> 
  tidybayes::add_epred_draws(
    fit,
    ndraws = 8000,
    allow_new_levels = TRUE
    ) |> 
  pivot_wider (
    names_from = .category,
    values_from = .epred
  ) |> 
  group_by(condition) |> 
  summarize(
    post_prob_hypothesis = mean(match > `non-match-1` & match > `non-match-2`)
  )
```

## More focused analysis: modal responses vs their predicted opposites

As a more focused analysis, we also look at whether participants pick the modal response option in conditions `low` and `high` more often than the opposite option, which is `high` in condition `low` and `low` in condition `high`.
To do this, we do *not* filter the data to only include trials in conditions `low` and `high`, but reuse the full model (with random effects), but we just hone in on what we predict to be the more pronounced contrasts:

```{r, BDA-posteriors-focused}
data_brm |> 
  dplyr::select(condition) |> 
  unique() |> 
  tidybayes::add_epred_draws(
    fit,
    ndraws = 8000,
    allow_new_levels = TRUE
    ) |> 
  pivot_wider (
    names_from = .category,
    values_from = .epred
  ) |> 
  filter(condition != "info") |>
  group_by(condition) |> 
  summarize(
    post_prob_hypothesis = mean(match > `non-match-2`)
  )
```
